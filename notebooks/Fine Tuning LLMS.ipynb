{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["XUWptaMmO84Q","q8mlUlYEuKov"],"authorship_tag":"ABX9TyOQwYJoIOpi+8iuPiVQcRLU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a029e29cc6394420b6d03ff5a95aa0b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9958f578eea411c983f172c906485b1","IPY_MODEL_5d0b8972f8024afdbc5eb1faba4f1c6b","IPY_MODEL_dd52fdebdb6840efb8e03a77e4bd03ca"],"layout":"IPY_MODEL_2dbc2cbdb0b44b6db8f374a0f67af8c1"}},"d9958f578eea411c983f172c906485b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_773b3fb0f1774b91b134ce52152798b8","placeholder":"​","style":"IPY_MODEL_b97b9e77d0414d8a96cffc69e543c4d9","value":"Loading checkpoint shards: 100%"}},"5d0b8972f8024afdbc5eb1faba4f1c6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fae05dfd8af04ed696ab2c7a8509fb89","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65a11cfb9f37472aa937e4f3fe1d1845","value":2}},"dd52fdebdb6840efb8e03a77e4bd03ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95118b46ab884eb9939fee4e92575ce7","placeholder":"​","style":"IPY_MODEL_f9e7473a08dc4ca998ebcc429b08b0bb","value":" 2/2 [00:55&lt;00:00, 25.30s/it]"}},"2dbc2cbdb0b44b6db8f374a0f67af8c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"773b3fb0f1774b91b134ce52152798b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b97b9e77d0414d8a96cffc69e543c4d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fae05dfd8af04ed696ab2c7a8509fb89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65a11cfb9f37472aa937e4f3fe1d1845":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95118b46ab884eb9939fee4e92575ce7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e7473a08dc4ca998ebcc429b08b0bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ee6f17e0b174ae4aa61bce0e890b89e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cac994974e104c7ba286dfa0a9e2b200","IPY_MODEL_749fd24c6fb44174a2a7ea7ae05a8400","IPY_MODEL_6742c66eb7ff458a8adb27bc34052aa5"],"layout":"IPY_MODEL_4cabda18c7504dc1923a3711b37259a0"}},"cac994974e104c7ba286dfa0a9e2b200":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_711cb9c221844fe194eafeb308324306","placeholder":"​","style":"IPY_MODEL_f5d95b740f2e4e3bb4d55f7f7ca88dda","value":"Map: 100%"}},"749fd24c6fb44174a2a7ea7ae05a8400":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_965de500d3f846cf9315438912e4a811","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e0cb316026944aca4ffa94d1daef347","value":200}},"6742c66eb7ff458a8adb27bc34052aa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39f9f2d8dd5d4b49a46f3717bb70144c","placeholder":"​","style":"IPY_MODEL_53507b5a774a4110a879da0f6841681b","value":" 200/200 [00:00&lt;00:00, 3048.40 examples/s]"}},"4cabda18c7504dc1923a3711b37259a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"711cb9c221844fe194eafeb308324306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5d95b740f2e4e3bb4d55f7f7ca88dda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"965de500d3f846cf9315438912e4a811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e0cb316026944aca4ffa94d1daef347":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39f9f2d8dd5d4b49a46f3717bb70144c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53507b5a774a4110a879da0f6841681b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef989e32075449f795a165afc0fd8394":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92930cfd5ab843cebd26a117587019ad","IPY_MODEL_a012c9622df744fbab4d99d3cf1238dc","IPY_MODEL_9dc2bea9271147f187465c724bb7916f"],"layout":"IPY_MODEL_25b50f9bb52e4714beb93339dd9958d6"}},"92930cfd5ab843cebd26a117587019ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ded65479c6e242238a45a028114d30a4","placeholder":"​","style":"IPY_MODEL_2dedf9821a1c4a76bf62d4cf86869517","value":"Map: 100%"}},"a012c9622df744fbab4d99d3cf1238dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5852142c0ed46cdb4025cc2d8becef1","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09a9a777649949d1b96ea49d80395752","value":200}},"9dc2bea9271147f187465c724bb7916f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5709fe832ac246b092bbbd868b2a0437","placeholder":"​","style":"IPY_MODEL_46969694b3e94f419852038fb8fe618f","value":" 200/200 [00:00&lt;00:00, 3141.41 examples/s]"}},"25b50f9bb52e4714beb93339dd9958d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded65479c6e242238a45a028114d30a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dedf9821a1c4a76bf62d4cf86869517":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5852142c0ed46cdb4025cc2d8becef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09a9a777649949d1b96ea49d80395752":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5709fe832ac246b092bbbd868b2a0437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46969694b3e94f419852038fb8fe618f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Finetuning Large Language Models for Emoji Generation"],"metadata":{"id":"iY7zMHulOxlC"}},{"cell_type":"code","source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 peft"],"metadata":{"collapsed":true,"id":"pNGaPmcVPwVt","executionInfo":{"status":"ok","timestamp":1733196444794,"user_tz":300,"elapsed":5887,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSjoyLq9QAo9","executionInfo":{"status":"ok","timestamp":1733195070952,"user_tz":300,"elapsed":20311,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"3023eb42-3a9e-4d45-bc31-91717e5ca022","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: read).\n","The token `Research` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `Research`\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"],"metadata":{"id":"5ADLBeZUUyR4","executionInfo":{"status":"ok","timestamp":1733195116806,"user_tz":300,"elapsed":40300,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Task 1 - Custom instruction dataset"],"metadata":{"id":"XUWptaMmO84Q"}},{"cell_type":"code","source":["data = [('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('I love reading books.', '📖❤️👓'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('I love reading books.', '📖❤️👓'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('I love reading books.', '📖❤️👓'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('I love reading books.', '📖❤️👓'),\n","('I love reading books.', '📖❤️👓'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('I love reading books.', '📖❤️👓'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('I love reading books.', '📖❤️👓'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('I love reading books.', '📖❤️👓'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Happy Birthday!', '🎉🎂🎈'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('It’s a beautiful sunny day!', '☀️🌻😎'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('I love reading books.', '📖❤️👓'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('I need a day to relax.', '🛀🧖\\u200d♀️😌'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Cooking a delicious meal.', '👩\\u200d🍳🥘🍴'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('Let’s go dancing tonight.', '💃🕺🎶'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Going on a road trip.', '🚗🗺️🛣️'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('Let’s grab a coffee.', '☕👥🤝'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Good luck on your interview!', '🍀👔💼'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Happy New Year!', '🎆🥂🎊'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('I’m listening to music.', '🎧🎶🎵'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('Working on my garden.', '👩\\u200d🌾🌿🌷'),\n","('Ready for the football game.', '🏈📣🏟️'),\n","('Feeling under the weather.', '🤒🛌🌧️'),\n","('Studying for exams.', '📚✍️🤓'),\n","('Time for a movie night.', '🍿🎬📺'),\n","('Enjoy your vacation!', '✈️🏖️🍹'),\n","('Have a great workout!', '💪🏋️\\u200d♂️🏃\\u200d♀️'),\n","('I love reading books.', '📖❤️👓'),\n","('Merry Christmas!', '🎄🎁❄️'),\n","('Congratulations on your graduation!', '👨\\u200d🎓🎓🥳')]\n"],"metadata":{"id":"pJzZ6ciaO7Y6","executionInfo":{"status":"ok","timestamp":1733195116807,"user_tz":300,"elapsed":4,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset\n","\n","dataset = Dataset.from_dict({\"text\": [f\"<s>[INST] {prompt} [/INST] {response}\" for prompt, response in data]})"],"metadata":{"id":"4ESThkQxPgGd","executionInfo":{"status":"ok","timestamp":1733197157334,"user_tz":300,"elapsed":139,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Task 2 - Finetune Llama2 and Show the results"],"metadata":{"id":"ruWMUh2YQnMN"}},{"cell_type":"code","source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-miniguanaco\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"],"metadata":{"id":"KTOCzkm4UoLz","executionInfo":{"status":"ok","timestamp":1733195116924,"user_tz":300,"elapsed":3,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["a029e29cc6394420b6d03ff5a95aa0b7","d9958f578eea411c983f172c906485b1","5d0b8972f8024afdbc5eb1faba4f1c6b","dd52fdebdb6840efb8e03a77e4bd03ca","2dbc2cbdb0b44b6db8f374a0f67af8c1","773b3fb0f1774b91b134ce52152798b8","b97b9e77d0414d8a96cffc69e543c4d9","fae05dfd8af04ed696ab2c7a8509fb89","65a11cfb9f37472aa937e4f3fe1d1845","95118b46ab884eb9939fee4e92575ce7","f9e7473a08dc4ca998ebcc429b08b0bb"]},"id":"JrW3XAilUpvy","executionInfo":{"status":"ok","timestamp":1733197031126,"user_tz":300,"elapsed":56460,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"a8714967-8ba1-4d09-f9d7-9281acf66a71"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a029e29cc6394420b6d03ff5a95aa0b7"}},"metadata":{}}]},{"cell_type":"code","source":["# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model and tokenizer\n","trainer.model.save_pretrained(new_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["2ee6f17e0b174ae4aa61bce0e890b89e","cac994974e104c7ba286dfa0a9e2b200","749fd24c6fb44174a2a7ea7ae05a8400","6742c66eb7ff458a8adb27bc34052aa5","4cabda18c7504dc1923a3711b37259a0","711cb9c221844fe194eafeb308324306","f5d95b740f2e4e3bb4d55f7f7ca88dda","965de500d3f846cf9315438912e4a811","8e0cb316026944aca4ffa94d1daef347","39f9f2d8dd5d4b49a46f3717bb70144c","53507b5a774a4110a879da0f6841681b"]},"id":"_iuwDyW3QsWa","executionInfo":{"status":"ok","timestamp":1733197205884,"user_tz":300,"elapsed":42992,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"836fc47c-acce-4e24-b2cd-964eac628889"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee6f17e0b174ae4aa61bce0e890b89e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50/50 00:39, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>2.706100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.111000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["import re\n","\n","def extract_emojis(text):\n","  emoji_pattern = re.compile(\"[\"\n","    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                       \"]+\", flags=re.UNICODE)\n","  return \"\".join(emoji_pattern.findall(text))"],"metadata":{"id":"ZyXir2E9oADs","executionInfo":{"status":"ok","timestamp":1733197571086,"user_tz":300,"elapsed":133,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["logging.set_verbosity(logging.CRITICAL)\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=28)\n","\n","# Known prompts\n","prompts = [\n","    'Going on a road trip.',\n","    \"Have a great workout!\",\n","    \"Let’s grab a coffee.\",\n","    \"I’m listening to music.\",\n","    \"I need a day to relax.\"\n","]\n","\n","# Generate emoji sequences for the prompts\n","for prompt in prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1X0IwsZZdOf","executionInfo":{"status":"ok","timestamp":1733198337223,"user_tz":300,"elapsed":14831,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"67f49324-0339-4427-b3b9-767ca3a1c1bd"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Going on a road trip.\n","Generated Emojis: 🚗🗺\n","\n","Prompt: Have a great workout!\n","Generated Emojis: 🏋\n","\n","Prompt: Let’s grab a coffee.\n","Generated Emojis: 👉🏼🍵\n","\n","Prompt: I’m listening to music.\n","Generated Emojis: 🎧🎶🎵\n","\n","Prompt: I need a day to relax.\n","Generated Emojis: 🛀\n","\n"]}]},{"cell_type":"markdown","source":["The model's performance on the known prompts shows a mix of accurate and inconsistent results. For the prompt \"Going on a road trip\" (🚗🗺), the model correctly used emojis conveying the concept of traveling and navigating. However, for \"Have a great workout!\" (🏋), it generated only a single emoji, which, while somewhat relevant, could have been enhanced by including additional emojis like \"💪\" or \"🏃‍♀️.\" For \"Let’s grab a coffee\" (👉🏼🍵), the model's use of the \"👉🏼\" emoji was not as fitting, and a better choice would have been \"☕\" or \"👥\" to convey the idea of meeting for coffee. On \"I’m listening to music\" (🎧🎶🎵), the model performed accurately and consistently, producing emojis that clearly represent music. Lastly, for \"I need a day to relax\" (🛀), while the model's output was reasonable, adding other relaxation-related emojis like 🧘‍♂️ or 🛋 could have provided more context. Overall, while the model was accurate for some prompts, it lacked consistency in others, particularly in fully capturing the intended idea or emoji variety."],"metadata":{"id":"0CSbGVoFq_OI"}},{"cell_type":"code","source":["novel_prompts = [\n","    \"Running at the beach\",\n","    \"I just signed a new job offer!\",\n","    \"Watching football with my friends.\",\n","    \"Go Irish!\",\n","    \"Going to win this race.\"\n","]\n","\n","for prompt in novel_prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1I-HIq5lnLma","executionInfo":{"status":"ok","timestamp":1733198593545,"user_tz":300,"elapsed":15389,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"6237d5e7-4d6b-45f4-b768-0709c12306ab"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Running at the beach\n","Generated Emojis: 🏃🌊🌴\n","\n","Prompt: I just signed a new job offer!\n","Generated Emojis: 💼👥💰\n","\n","Prompt: Watching football with my friends.\n","Generated Emojis: 🏈👥🍔\n","\n","Prompt: Go Irish!\n","Generated Emojis: 🍀🍀🍀🍀\n","\n","Prompt: Going to win this race.\n","Generated Emojis: 🏎💨🏁\n","\n"]}]},{"cell_type":"markdown","source":["The model's ability to generalize beyond the training data shows promising results, though with some areas for improvement. For the prompt \"Running at the beach\" (🏃🌊🌴), the model correctly generated relevant emojis representing running and the beach, which is an accurate output. In response to \"I just signed a new job offer!\" (💼👥💰), the model provided appropriate emojis related to work and money, though a \"✍️\" emoji for signing would have added more context. For \"Watching football with my friends\" (🏈👥🍔), the model produced a reasonable set of emojis, though it could have included \"🎉\" or \"🍻\" to better reflect a fun social atmosphere. The prompt \"Go Irish!\" (🍀🍀🍀🍀) was handled well, with the model accurately using the shamrock emoji, though the repetition of the emoji may have been excessive. Lastly, for \"Going to win this race\" (🏎💨🏁), the model generated emojis that clearly represent racing, demonstrating a good understanding of the prompt. Overall, the model was able to generalize well to novel prompts, though it could have benefitted from a bit more variety or context in some outputs."],"metadata":{"id":"yr6tWc5ysJJo"}},{"cell_type":"code","source":["edge_case_prompts = [\n","    \"Its night in the daytime.\",\n","    \"I am crying of joy.\",\n","    \"Excited and scared for this new job.\",\n","    \"It's warm and snowing.\",\n","    \"I hate her and I love her.\"\n","]\n","\n","for prompt in edge_case_prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yF0d-grnRg2","executionInfo":{"status":"ok","timestamp":1733199016165,"user_tz":300,"elapsed":11839,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"3482fc9d-9099-4db6-c7cd-3b406fc25097"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Its night in the daytime.\n","Generated Emojis: 🌃🕰\n","\n","Prompt: I am crying of joy.\n","Generated Emojis: 😭🎉💕\n","\n","Prompt: Excited and scared for this new job.\n","Generated Emojis: 😃😨\n","\n","Prompt: It's warm and snowing.\n","Generated Emojis: 🌞\n","\n","Prompt: I hate her and I love her.\n","Generated Emojis: 😠💕\n","\n"]}]},{"cell_type":"markdown","source":["The model's handling of confusing or contradictory prompts shows a mixture of reasonable outputs and notable failures. For \"It's night in the daytime\" (🌃🕰), the model correctly used the night sky emoji but chose a clock emoji (🕰) instead of something more fitting for daytime, like a sun or day symbol. In response to \"I am crying of joy\" (😭🎉💕), the model accurately used the crying emoji and joyful symbols, effectively capturing the emotional contrast of the prompt. For \"Excited and scared for this new job\" (😃😨), the model generated an appropriate mix of excitement and fear, though it could have included more nuanced emojis to better reflect the complexity of the emotions. The output for \"It's warm and snowing\" (🌞) was a failure, as it only included the sun emoji, ignoring the contradictory snow aspect. Finally, for \"I hate her and I love her\" (😠💕), the model generated emojis that capture the conflicting emotions, but it could have better expressed the contradiction with more emojis, such as a combination of love and anger symbols. Overall, while the model handled some contradictory prompts well, it struggled with fully capturing the complexity or contradiction in others."],"metadata":{"id":"560E8KPstu0b"}},{"cell_type":"markdown","source":["## Task 3 - Comparison with GPT 4"],"metadata":{"id":"q8mlUlYEuKov"}},{"cell_type":"markdown","source":["### ChatGPT Prompts:\n","1. **Running at the beach**  \n","   *Prompt:* Generate only emojis that symbolize running at the beach.\n","   \n","2. **I just signed a new job offer!**  \n","   *Prompt:* Respond with emojis that capture the excitement of signing a new job offer.\n","   \n","3. **Watching football with my friends.**  \n","   *Prompt:* Generate only emojis that represent watching football with friends.\n","\n","4. **Go Irish!**  \n","   *Prompt:* Respond with emojis that represent the phrase \"Go Irish!\".\n","   \n","5. **Going to win this race.**  \n","   *Prompt:* Generate only emojis that represent the feeling of going to win a race.\n","\n","### Fine-Tuned LLaMA 2 Model Outputs:\n","1. **Running at the beach**  \n","   *Output:* 🏃🌊🌴  \n","   \n","2. **I just signed a new job offer!**  \n","   *Output:* 💼👥💰  \n","\n","3. **Watching football with my friends**  \n","   *Output:* 🏈👥🍔  \n","\n","4. **Go Irish!**  \n","   *Output:* 🍀🍀🍀🍀  \n","\n","5. **Going to win this race**  \n","   *Output:* 🏎💨🏁\n","\n","### ChatGPT Outputs:\n","1. **Running at the beach**  \n","   *Output:* 🏃‍♀️🌊☀️  \n","\n","2. **I just signed a new job offer!**  \n","   *Output:* 🖋️💼🎉  \n","\n","3. **Watching football with my friends**  \n","   *Output:* 🏈🍺👯  \n","\n","4. **Go Irish!**  \n","   *Output:* 🍀🇮🇪🎉  \n","\n","5. **Going to win this race**  \n","   *Output:* 🏁💨🏆\n","\n","### Comparison:\n","\n","1. **Running at the beach:**  \n","   - **LLaMA 2:** 🏃🌊🌴  \n","   - **ChatGPT:** 🏃‍♀️🌊☀️  \n","   - **Analysis:** Both models generated appropriate emojis. LLaMA 2 used a simple and accurate set of running, beach, and nature-related emojis. ChatGPT added a sun emoji, making it more specific to a daytime beach scenario, which could be a more fitting context depending on the user's intent.  \n","   - **Quality & Relevance:** Both outputs are relevant, but ChatGPT adds more context with the sun emoji.\n","\n","2. **I just signed a new job offer:**  \n","   - **LLaMA 2:** 💼👥💰  \n","   - **ChatGPT:** 🖋️💼🎉  \n","   - **Analysis:** LLaMA 2 focused on work and money-related emojis, which are relevant but could be seen as more formal. ChatGPT added the pen emoji (🖋️), representing the signing aspect, and included the celebratory \"🎉,\" which adds emotional context.  \n","   - **Quality & Relevance:** Both are relevant, but ChatGPT's inclusion of the pen and celebration makes the output more fitting for the excitement of signing a job offer.\n","\n","3. **Watching football with my friends:**  \n","   - **LLaMA 2:** 🏈👥🍔  \n","   - **ChatGPT:** 🏈🍺👯  \n","   - **Analysis:** LLaMA 2 provided a more neutral, straightforward approach, focusing on football, friends, and food. ChatGPT's output included \"🍺,\" which emphasizes the social aspect of watching a game, and \"👯,\" possibly symbolizing friendship or a group gathering.  \n","   - **Quality & Relevance:** Both outputs are good, but ChatGPT adds more specific social context with the drink and gathering emojis.\n","\n","4. **Go Irish!**  \n","   - **LLaMA 2:** 🍀🍀🍀🍀  \n","   - **ChatGPT:** 🍀🇮🇪🎉  \n","   - **Analysis:** LLaMA 2 used repeated shamrocks to symbolize \"Go Irish!\" ChatGPT added more variety by including the Irish flag emoji and a celebration emoji.  \n","   - **Quality & Relevance:** LLaMA 2’s response is a bit repetitive, while ChatGPT’s is more creative and includes a wider range of symbols.\n","\n","5. **Going to win this race:**  \n","   - **LLaMA 2:** 🏎💨🏁  \n","   - **ChatGPT:** 🏁💨🏆  \n","   - **Analysis:** Both models captured the idea of racing well. LLaMA 2 used a race car, speed, and finish line, which is highly relevant. ChatGPT used the finish line, speed, and added the trophy emoji (🏆), emphasizing the victory aspect.  \n","   - **Quality & Relevance:** Both outputs are strong, but ChatGPT's inclusion of the trophy better emphasizes the theme of winning.\n","\n","Both generate relevant and creative emoji sequences, though ChatGPT's outputs tend to have more context, variety, and creativity in some cases."],"metadata":{"id":"JdR83fxDuOQK"}},{"cell_type":"markdown","source":[],"metadata":{"id":"f3kAXFW6uceV"}},{"cell_type":"markdown","source":["## Task 4: Investigating Overfitting in Fine-Tuning"],"metadata":{"id":"vhbkLsvVu2Dh"}},{"cell_type":"code","source":["num_train_epochs = 5\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model and tokenizer\n","trainer.model.save_pretrained(new_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337,"referenced_widgets":["ef989e32075449f795a165afc0fd8394","92930cfd5ab843cebd26a117587019ad","a012c9622df744fbab4d99d3cf1238dc","9dc2bea9271147f187465c724bb7916f","25b50f9bb52e4714beb93339dd9958d6","ded65479c6e242238a45a028114d30a4","2dedf9821a1c4a76bf62d4cf86869517","a5852142c0ed46cdb4025cc2d8becef1","09a9a777649949d1b96ea49d80395752","5709fe832ac246b092bbbd868b2a0437","46969694b3e94f419852038fb8fe618f"]},"id":"-yIuhpa_vXZH","executionInfo":{"status":"ok","timestamp":1733199741055,"user_tz":300,"elapsed":213552,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"944ec80d-64dd-4773-8d81-47b70a12ac1a"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef989e32075449f795a165afc0fd8394"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'loss': 2.9724, 'learning_rate': 0.0001975746552556772, 'epoch': 0.5}\n","{'loss': 1.0541, 'learning_rate': 0.00018550053929480202, 'epoch': 1.0}\n","{'loss': 0.4644, 'learning_rate': 0.00016449948488669639, 'epoch': 1.5}\n","{'loss': 0.2749, 'learning_rate': 0.000136764169663272, 'epoch': 2.0}\n","{'loss': 0.1908, 'learning_rate': 0.00010519038181318999, 'epoch': 2.5}\n","{'loss': 0.179, 'learning_rate': 7.307467669163655e-05, 'epoch': 3.0}\n","{'loss': 0.1649, 'learning_rate': 4.377019014049223e-05, 'epoch': 3.5}\n","{'loss': 0.1623, 'learning_rate': 2.03365443542764e-05, 'epoch': 4.0}\n","{'loss': 0.156, 'learning_rate': 5.22039891260262e-06, 'epoch': 4.5}\n","{'loss': 0.1574, 'learning_rate': 0.0, 'epoch': 5.0}\n","{'train_runtime': 210.884, 'train_samples_per_second': 4.742, 'train_steps_per_second': 1.185, 'train_loss': 0.5776116104125977, 'epoch': 5.0}\n"]}]},{"cell_type":"code","source":["pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=28)\n","\n","prompts = [\n","    'Have a great workout!',\n","    \"Running at the beach.\",\n","    \"Happy holidays!\",\n","    \"I just signed a new job offer!\",\n","    \"I’m listening to music.\"\n","]\n","\n","# Generate emoji sequences for the prompts\n","for prompt in prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbN6DBl6vhJX","executionInfo":{"status":"ok","timestamp":1733200423195,"user_tz":300,"elapsed":14290,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"5c359671-1613-4a3d-8998-68c8cb177424"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Have a great workout!\n","Generated Emojis: 💪🏋\n","\n","Prompt: Running at the beach.\n","Generated Emojis: 🏃🏖\n","\n","Prompt: Happy holidays!\n","Generated Emojis: 🎄🎁\n","\n","Prompt: I just signed a new job offer!\n","Generated Emojis: 👨\n","\n","Prompt: I’m listening to music.\n","Generated Emojis: 🎧🎶🎵\n","\n"]}]},{"cell_type":"markdown","source":["After fine-tuning the model for 5 epochs, the generated emojis for the given prompts are mostly relevant but show some inconsistencies. For \"Have a great workout!\" the emojis 💪🏋 are fitting, though the addition of a person exercising might improve it. \"Running at the beach\" with 🏃🏖 appropriately conveys the activity and setting. \"Happy holidays!\" with 🎄🎁 captures the festive spirit well. \"I just signed a new job offer!\" is less precise with just 👨, missing out on symbols like a briefcase or money, which would be more relevant. Finally, \"I’m listening to music\" with 🎧🎶🎵 is spot-on. Overall, while the model produces contextually relevant emojis, some prompts could benefit from more precise or varied outputs."],"metadata":{"id":"Ppb08QCMx1uA"}},{"cell_type":"code","source":["pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n","\n","prompts = [\n","    'Explain the concept of gravity.',\n","    \"What is the capital of the USA.\",\n","    \"What is the largest planet in the solar system?\",\n","    \"Who created Instagram?\",\n","    \"What is the chemical symbol for water\"\n","]\n","\n","# Generate emoji sequences for the prompts\n","for prompt in prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Text: {result[0]['generated_text']}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4jOraAHx2dG","executionInfo":{"status":"ok","timestamp":1733200515536,"user_tz":300,"elapsed":83784,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"c631b210-fbf6-45b1-b296-c5a58f94b31e"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Explain the concept of gravity.\n","Generated Text: <s>[INST] Explain the concept of gravity. [/INST] 🌟👀🌲🌊🛌😌👍🤓🚀🌟🌊🐳🌟😍👀🤓🛌😌👍🤓🚀\n","\n","Prompt: What is the capital of the USA.\n","Generated Text: <s>[INST] What is the capital of the USA. [/INST] 🇺🇸🗺️🏛️🇬🇧\n","\n","Prompt: What is the largest planet in the solar system?\n","Generated Text: <s>[INST] What is the largest planet in the solar system? [/INST] 🌐👥🚀🌌👀🤔💭🌟🌊🏜️🛌😌👍🤩🚫🛫🎉🤩👏👌\n","\n","Prompt: Who created Instagram?\n","Generated Text: <s>[INST] Who created Instagram? [/INST] 📷👓🤝 [/INST] 📚🎓🥳 [/INST] 🎧🎮👥👦🏼🚣🏼🛀👀🤝🏼😍 [/INST]\n","\n","Prompt: What is the chemical symbol for water\n","Generated Text: <s>[INST] What is the chemical symbol for water [/INST] 💧🌊🥂\n"," everybody! 🎉🎊🎈🎉🎊🎈🎉🎊🎈🎉🎊🎈🎉🎊🎈🎉🎊\n","\n"]}]},{"cell_type":"markdown","source":["\n","1. **Gravity :**\n","   - **Fine-Tuned Model:** The fine-tuned model generates a series of emojis that don't align with the expected explanation of gravity, such as 🌟👀🌲, and completely fails to provide a coherent response related to the concept of gravity.\n","   - **Original Model:** The original model provides a detailed and accurate scientific explanation of gravity as a force of nature.\n","\n","The fine-tuned model strays far from the expected factual answer, producing an incoherent string of emojis instead of the informative content expected. This could indicate overfitting to the emoji generation task, causing it to default to emojis even when not appropriate.\n","<br> <br>\n","\n","2. **Capital of the USA:**\n","   - **Fine-Tuned Model:** The fine-tuned model generates emojis like 🇺🇸🗺️🏛️🇬🇧, which, although they include some relevant symbols (e.g., 🇺🇸 for the USA and 🏛️ for a government building), fail to specify the capital, Washington, D.C.\n","   - **Original Model:** The original model provides a clear and accurate response, stating that Washington, D.C. is the capital of the USA.\n","   \n","The fine-tuned model fails to provide the correct factual information, replacing it with emojis. This suggests a loss of generalization ability, where the model no longer effectively handles factual questions after fine-tuning.\n","\n","<br><br>\n","\n","3. **Largest Planet in the Solar System:**\n","   - **Fine-Tuned Model:** The fine-tuned model outputs a string of emojis that don't convey any information about the largest planet, Jupiter. Instead, it produces a random sequence of emojis like 🌐👥🚀.\n","   - **Original Model:** The original model correctly identifies Jupiter as the largest planet in the solar system and provides supporting details.\n","\n","This response highlights the model's failure to provide a coherent or useful answer post-fine-tuning, focusing instead on irrelevant emojis. This further suggests overfitting to the emoji generation task, where the model has lost its ability to process and respond appropriately to factual queries.\n","<br><br>\n","\n","4. **Who Created Instagram:**\n","   - **Fine-Tuned Model:** The fine-tuned model provides a disjointed series of emojis (e.g., 📷👓🤝), which don't meaningfully address the question of who created Instagram.\n","   - **Original Model:** The original model offers a comprehensive response, detailing the founders of Instagram and their timeline.\n","  \n","  The fine-tuned model's response is entirely out of context, showing that it now prioritizes emoji generation over factual accuracy, diverging from the expected behavior of providing factual answers.\n","  <br><br>\n","\n","5. **Chemical Symbol for Water:**\n","   - **Fine-Tuned Model:** The fine-tuned model produces emojis like 💧🌊🥂 and a long string of celebration-related emojis, which fail to provide the correct answer (H2O).\n","   - **Original Model:** The original model accurately states that the chemical symbol for water is H2O.\n","  \n","  Once again, the fine-tuned model deviates from providing accurate information, instead generating irrelevant emojis and disrupting the original model's ability to answer a straightforward question.\n","  <br>\n","  <br>\n","\n","\n","The fine-tuned model struggles to handle factual prompts, producing irrelevant or nonsensical emoji sequences instead of providing the expected, informative answers. This  contrast to the original LLaMA 2 model's performance indicates overfitting to the emoji generation task, where the model fails to generalize properly and loses its ability to provide factual or contextually appropriate responses."],"metadata":{"id":"wIHVMRdv0LBf"}},{"cell_type":"markdown","source":["\n","The fine-tuned model shows clear signs of overfitting, particularly in its inability to generate relevant text-based answers for factual prompts. Instead of providing informative responses to prompts like \"Explain the concept of gravity\" or \"What is the capital of the USA?\", the model consistently produces irrelevant emoji sequences. This suggests that the model has become too specialized in the emoji generation task and has lost its ability to generalize to other types of queries. Furthermore, the emoji sequences generated are repetitive and lack creativity, indicating that the model is overly focused on the learned emoji patterns.\n","\n","To address overfitting, several strategies could be employed. Early stopping could prevent the model from training too long, ensuring it doesn't overfit to the training data and retains its ability to generalize. Regularization techniques like L2 regularization or dropout would encourage the model to learn more generalized patterns rather than memorizing specific responses. Additionally, data augmentation could help by diversifying the training set, making the model less likely to overfit to specific emoji sequences. Finally, hyperparameter tuning could optimize the learning process, ensuring the model learns at a balanced pace and doesn't become too specialized in the emoji task. By combining these strategies, the model could be fine-tuned to improve both its performance in emoji generation and its ability to handle a variety of tasks effectively."],"metadata":{"id":"uTNcqs4d0ukh"}}]}