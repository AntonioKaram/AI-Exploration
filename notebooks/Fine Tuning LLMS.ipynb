{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["XUWptaMmO84Q","q8mlUlYEuKov"],"authorship_tag":"ABX9TyOQwYJoIOpi+8iuPiVQcRLU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a029e29cc6394420b6d03ff5a95aa0b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9958f578eea411c983f172c906485b1","IPY_MODEL_5d0b8972f8024afdbc5eb1faba4f1c6b","IPY_MODEL_dd52fdebdb6840efb8e03a77e4bd03ca"],"layout":"IPY_MODEL_2dbc2cbdb0b44b6db8f374a0f67af8c1"}},"d9958f578eea411c983f172c906485b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_773b3fb0f1774b91b134ce52152798b8","placeholder":"â€‹","style":"IPY_MODEL_b97b9e77d0414d8a96cffc69e543c4d9","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"5d0b8972f8024afdbc5eb1faba4f1c6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fae05dfd8af04ed696ab2c7a8509fb89","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65a11cfb9f37472aa937e4f3fe1d1845","value":2}},"dd52fdebdb6840efb8e03a77e4bd03ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95118b46ab884eb9939fee4e92575ce7","placeholder":"â€‹","style":"IPY_MODEL_f9e7473a08dc4ca998ebcc429b08b0bb","value":"â€‡2/2â€‡[00:55&lt;00:00,â€‡25.30s/it]"}},"2dbc2cbdb0b44b6db8f374a0f67af8c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"773b3fb0f1774b91b134ce52152798b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b97b9e77d0414d8a96cffc69e543c4d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fae05dfd8af04ed696ab2c7a8509fb89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65a11cfb9f37472aa937e4f3fe1d1845":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95118b46ab884eb9939fee4e92575ce7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e7473a08dc4ca998ebcc429b08b0bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ee6f17e0b174ae4aa61bce0e890b89e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cac994974e104c7ba286dfa0a9e2b200","IPY_MODEL_749fd24c6fb44174a2a7ea7ae05a8400","IPY_MODEL_6742c66eb7ff458a8adb27bc34052aa5"],"layout":"IPY_MODEL_4cabda18c7504dc1923a3711b37259a0"}},"cac994974e104c7ba286dfa0a9e2b200":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_711cb9c221844fe194eafeb308324306","placeholder":"â€‹","style":"IPY_MODEL_f5d95b740f2e4e3bb4d55f7f7ca88dda","value":"Map:â€‡100%"}},"749fd24c6fb44174a2a7ea7ae05a8400":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_965de500d3f846cf9315438912e4a811","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e0cb316026944aca4ffa94d1daef347","value":200}},"6742c66eb7ff458a8adb27bc34052aa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39f9f2d8dd5d4b49a46f3717bb70144c","placeholder":"â€‹","style":"IPY_MODEL_53507b5a774a4110a879da0f6841681b","value":"â€‡200/200â€‡[00:00&lt;00:00,â€‡3048.40â€‡examples/s]"}},"4cabda18c7504dc1923a3711b37259a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"711cb9c221844fe194eafeb308324306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5d95b740f2e4e3bb4d55f7f7ca88dda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"965de500d3f846cf9315438912e4a811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e0cb316026944aca4ffa94d1daef347":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39f9f2d8dd5d4b49a46f3717bb70144c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53507b5a774a4110a879da0f6841681b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef989e32075449f795a165afc0fd8394":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92930cfd5ab843cebd26a117587019ad","IPY_MODEL_a012c9622df744fbab4d99d3cf1238dc","IPY_MODEL_9dc2bea9271147f187465c724bb7916f"],"layout":"IPY_MODEL_25b50f9bb52e4714beb93339dd9958d6"}},"92930cfd5ab843cebd26a117587019ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ded65479c6e242238a45a028114d30a4","placeholder":"â€‹","style":"IPY_MODEL_2dedf9821a1c4a76bf62d4cf86869517","value":"Map:â€‡100%"}},"a012c9622df744fbab4d99d3cf1238dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5852142c0ed46cdb4025cc2d8becef1","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09a9a777649949d1b96ea49d80395752","value":200}},"9dc2bea9271147f187465c724bb7916f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5709fe832ac246b092bbbd868b2a0437","placeholder":"â€‹","style":"IPY_MODEL_46969694b3e94f419852038fb8fe618f","value":"â€‡200/200â€‡[00:00&lt;00:00,â€‡3141.41â€‡examples/s]"}},"25b50f9bb52e4714beb93339dd9958d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded65479c6e242238a45a028114d30a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dedf9821a1c4a76bf62d4cf86869517":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5852142c0ed46cdb4025cc2d8becef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09a9a777649949d1b96ea49d80395752":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5709fe832ac246b092bbbd868b2a0437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46969694b3e94f419852038fb8fe618f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Finetuning Large Language Models for Emoji Generation"],"metadata":{"id":"iY7zMHulOxlC"}},{"cell_type":"code","source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 peft"],"metadata":{"collapsed":true,"id":"pNGaPmcVPwVt","executionInfo":{"status":"ok","timestamp":1733196444794,"user_tz":300,"elapsed":5887,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSjoyLq9QAo9","executionInfo":{"status":"ok","timestamp":1733195070952,"user_tz":300,"elapsed":20311,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"3023eb42-3a9e-4d45-bc31-91717e5ca022","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: read).\n","The token `Research` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `Research`\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"],"metadata":{"id":"5ADLBeZUUyR4","executionInfo":{"status":"ok","timestamp":1733195116806,"user_tz":300,"elapsed":40300,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Task 1 - Custom instruction dataset"],"metadata":{"id":"XUWptaMmO84Q"}},{"cell_type":"code","source":["data = [('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Happy Birthday!', 'ğŸ‰ğŸ‚ğŸˆ'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Itâ€™s a beautiful sunny day!', 'â˜€ï¸ğŸŒ»ğŸ˜'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('I need a day to relax.', 'ğŸ›€ğŸ§–\\u200dâ™€ï¸ğŸ˜Œ'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Cooking a delicious meal.', 'ğŸ‘©\\u200dğŸ³ğŸ¥˜ğŸ´'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Letâ€™s go dancing tonight.', 'ğŸ’ƒğŸ•ºğŸ¶'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Going on a road trip.', 'ğŸš—ğŸ—ºï¸ğŸ›£ï¸'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('Letâ€™s grab a coffee.', 'â˜•ğŸ‘¥ğŸ¤'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Good luck on your interview!', 'ğŸ€ğŸ‘”ğŸ’¼'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Happy New Year!', 'ğŸ†ğŸ¥‚ğŸŠ'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Iâ€™m listening to music.', 'ğŸ§ğŸ¶ğŸµ'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('Working on my garden.', 'ğŸ‘©\\u200dğŸŒ¾ğŸŒ¿ğŸŒ·'),\n","('Ready for the football game.', 'ğŸˆğŸ“£ğŸŸï¸'),\n","('Feeling under the weather.', 'ğŸ¤’ğŸ›ŒğŸŒ§ï¸'),\n","('Studying for exams.', 'ğŸ“šâœï¸ğŸ¤“'),\n","('Time for a movie night.', 'ğŸ¿ğŸ¬ğŸ“º'),\n","('Enjoy your vacation!', 'âœˆï¸ğŸ–ï¸ğŸ¹'),\n","('Have a great workout!', 'ğŸ’ªğŸ‹ï¸\\u200dâ™‚ï¸ğŸƒ\\u200dâ™€ï¸'),\n","('I love reading books.', 'ğŸ“–â¤ï¸ğŸ‘“'),\n","('Merry Christmas!', 'ğŸ„ğŸâ„ï¸'),\n","('Congratulations on your graduation!', 'ğŸ‘¨\\u200dğŸ“ğŸ“ğŸ¥³')]\n"],"metadata":{"id":"pJzZ6ciaO7Y6","executionInfo":{"status":"ok","timestamp":1733195116807,"user_tz":300,"elapsed":4,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset\n","\n","dataset = Dataset.from_dict({\"text\": [f\"<s>[INST] {prompt} [/INST] {response}\" for prompt, response in data]})"],"metadata":{"id":"4ESThkQxPgGd","executionInfo":{"status":"ok","timestamp":1733197157334,"user_tz":300,"elapsed":139,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Task 2 - Finetune Llama2 and Show the results"],"metadata":{"id":"ruWMUh2YQnMN"}},{"cell_type":"code","source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","\n","# Fine-tuned model name\n","new_model = \"llama-2-7b-miniguanaco\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"],"metadata":{"id":"KTOCzkm4UoLz","executionInfo":{"status":"ok","timestamp":1733195116924,"user_tz":300,"elapsed":3,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["a029e29cc6394420b6d03ff5a95aa0b7","d9958f578eea411c983f172c906485b1","5d0b8972f8024afdbc5eb1faba4f1c6b","dd52fdebdb6840efb8e03a77e4bd03ca","2dbc2cbdb0b44b6db8f374a0f67af8c1","773b3fb0f1774b91b134ce52152798b8","b97b9e77d0414d8a96cffc69e543c4d9","fae05dfd8af04ed696ab2c7a8509fb89","65a11cfb9f37472aa937e4f3fe1d1845","95118b46ab884eb9939fee4e92575ce7","f9e7473a08dc4ca998ebcc429b08b0bb"]},"id":"JrW3XAilUpvy","executionInfo":{"status":"ok","timestamp":1733197031126,"user_tz":300,"elapsed":56460,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"a8714967-8ba1-4d09-f9d7-9281acf66a71"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a029e29cc6394420b6d03ff5a95aa0b7"}},"metadata":{}}]},{"cell_type":"code","source":["# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model and tokenizer\n","trainer.model.save_pretrained(new_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["2ee6f17e0b174ae4aa61bce0e890b89e","cac994974e104c7ba286dfa0a9e2b200","749fd24c6fb44174a2a7ea7ae05a8400","6742c66eb7ff458a8adb27bc34052aa5","4cabda18c7504dc1923a3711b37259a0","711cb9c221844fe194eafeb308324306","f5d95b740f2e4e3bb4d55f7f7ca88dda","965de500d3f846cf9315438912e4a811","8e0cb316026944aca4ffa94d1daef347","39f9f2d8dd5d4b49a46f3717bb70144c","53507b5a774a4110a879da0f6841681b"]},"id":"_iuwDyW3QsWa","executionInfo":{"status":"ok","timestamp":1733197205884,"user_tz":300,"elapsed":42992,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"836fc47c-acce-4e24-b2cd-964eac628889"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee6f17e0b174ae4aa61bce0e890b89e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [50/50 00:39, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>2.706100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.111000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["import re\n","\n","def extract_emojis(text):\n","  emoji_pattern = re.compile(\"[\"\n","    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                       \"]+\", flags=re.UNICODE)\n","  return \"\".join(emoji_pattern.findall(text))"],"metadata":{"id":"ZyXir2E9oADs","executionInfo":{"status":"ok","timestamp":1733197571086,"user_tz":300,"elapsed":133,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["logging.set_verbosity(logging.CRITICAL)\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=28)\n","\n","# Known prompts\n","prompts = [\n","    'Going on a road trip.',\n","    \"Have a great workout!\",\n","    \"Letâ€™s grab a coffee.\",\n","    \"Iâ€™m listening to music.\",\n","    \"I need a day to relax.\"\n","]\n","\n","# Generate emoji sequences for the prompts\n","for prompt in prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1X0IwsZZdOf","executionInfo":{"status":"ok","timestamp":1733198337223,"user_tz":300,"elapsed":14831,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"67f49324-0339-4427-b3b9-767ca3a1c1bd"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Going on a road trip.\n","Generated Emojis: ğŸš—ğŸ—º\n","\n","Prompt: Have a great workout!\n","Generated Emojis: ğŸ‹\n","\n","Prompt: Letâ€™s grab a coffee.\n","Generated Emojis: ğŸ‘‰ğŸ¼ğŸµ\n","\n","Prompt: Iâ€™m listening to music.\n","Generated Emojis: ğŸ§ğŸ¶ğŸµ\n","\n","Prompt: I need a day to relax.\n","Generated Emojis: ğŸ›€\n","\n"]}]},{"cell_type":"markdown","source":["The model's performance on the known prompts shows a mix of accurate and inconsistent results. For the prompt \"Going on a road trip\" (ğŸš—ğŸ—º), the model correctly used emojis conveying the concept of traveling and navigating. However, for \"Have a great workout!\" (ğŸ‹), it generated only a single emoji, which, while somewhat relevant, could have been enhanced by including additional emojis like \"ğŸ’ª\" or \"ğŸƒâ€â™€ï¸.\" For \"Letâ€™s grab a coffee\" (ğŸ‘‰ğŸ¼ğŸµ), the model's use of the \"ğŸ‘‰ğŸ¼\" emoji was not as fitting, and a better choice would have been \"â˜•\" or \"ğŸ‘¥\" to convey the idea of meeting for coffee. On \"Iâ€™m listening to music\" (ğŸ§ğŸ¶ğŸµ), the model performed accurately and consistently, producing emojis that clearly represent music. Lastly, for \"I need a day to relax\" (ğŸ›€), while the model's output was reasonable, adding other relaxation-related emojis like ğŸ§˜â€â™‚ï¸ or ğŸ›‹ could have provided more context. Overall, while the model was accurate for some prompts, it lacked consistency in others, particularly in fully capturing the intended idea or emoji variety."],"metadata":{"id":"0CSbGVoFq_OI"}},{"cell_type":"code","source":["novel_prompts = [\n","    \"Running at the beach\",\n","    \"I just signed a new job offer!\",\n","    \"Watching football with my friends.\",\n","    \"Go Irish!\",\n","    \"Going to win this race.\"\n","]\n","\n","for prompt in novel_prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1I-HIq5lnLma","executionInfo":{"status":"ok","timestamp":1733198593545,"user_tz":300,"elapsed":15389,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"6237d5e7-4d6b-45f4-b768-0709c12306ab"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Running at the beach\n","Generated Emojis: ğŸƒğŸŒŠğŸŒ´\n","\n","Prompt: I just signed a new job offer!\n","Generated Emojis: ğŸ’¼ğŸ‘¥ğŸ’°\n","\n","Prompt: Watching football with my friends.\n","Generated Emojis: ğŸˆğŸ‘¥ğŸ”\n","\n","Prompt: Go Irish!\n","Generated Emojis: ğŸ€ğŸ€ğŸ€ğŸ€\n","\n","Prompt: Going to win this race.\n","Generated Emojis: ğŸğŸ’¨ğŸ\n","\n"]}]},{"cell_type":"markdown","source":["The model's ability to generalize beyond the training data shows promising results, though with some areas for improvement. For the prompt \"Running at the beach\" (ğŸƒğŸŒŠğŸŒ´), the model correctly generated relevant emojis representing running and the beach, which is an accurate output. In response to \"I just signed a new job offer!\" (ğŸ’¼ğŸ‘¥ğŸ’°), the model provided appropriate emojis related to work and money, though a \"âœï¸\" emoji for signing would have added more context. For \"Watching football with my friends\" (ğŸˆğŸ‘¥ğŸ”), the model produced a reasonable set of emojis, though it could have included \"ğŸ‰\" or \"ğŸ»\" to better reflect a fun social atmosphere. The prompt \"Go Irish!\" (ğŸ€ğŸ€ğŸ€ğŸ€) was handled well, with the model accurately using the shamrock emoji, though the repetition of the emoji may have been excessive. Lastly, for \"Going to win this race\" (ğŸğŸ’¨ğŸ), the model generated emojis that clearly represent racing, demonstrating a good understanding of the prompt. Overall, the model was able to generalize well to novel prompts, though it could have benefitted from a bit more variety or context in some outputs."],"metadata":{"id":"yr6tWc5ysJJo"}},{"cell_type":"code","source":["edge_case_prompts = [\n","    \"Its night in the daytime.\",\n","    \"I am crying of joy.\",\n","    \"Excited and scared for this new job.\",\n","    \"It's warm and snowing.\",\n","    \"I hate her and I love her.\"\n","]\n","\n","for prompt in edge_case_prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yF0d-grnRg2","executionInfo":{"status":"ok","timestamp":1733199016165,"user_tz":300,"elapsed":11839,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"3482fc9d-9099-4db6-c7cd-3b406fc25097"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Its night in the daytime.\n","Generated Emojis: ğŸŒƒğŸ•°\n","\n","Prompt: I am crying of joy.\n","Generated Emojis: ğŸ˜­ğŸ‰ğŸ’•\n","\n","Prompt: Excited and scared for this new job.\n","Generated Emojis: ğŸ˜ƒğŸ˜¨\n","\n","Prompt: It's warm and snowing.\n","Generated Emojis: ğŸŒ\n","\n","Prompt: I hate her and I love her.\n","Generated Emojis: ğŸ˜ ğŸ’•\n","\n"]}]},{"cell_type":"markdown","source":["The model's handling of confusing or contradictory prompts shows a mixture of reasonable outputs and notable failures. For \"It's night in the daytime\" (ğŸŒƒğŸ•°), the model correctly used the night sky emoji but chose a clock emoji (ğŸ•°) instead of something more fitting for daytime, like a sun or day symbol. In response to \"I am crying of joy\" (ğŸ˜­ğŸ‰ğŸ’•), the model accurately used the crying emoji and joyful symbols, effectively capturing the emotional contrast of the prompt. For \"Excited and scared for this new job\" (ğŸ˜ƒğŸ˜¨), the model generated an appropriate mix of excitement and fear, though it could have included more nuanced emojis to better reflect the complexity of the emotions. The output for \"It's warm and snowing\" (ğŸŒ) was a failure, as it only included the sun emoji, ignoring the contradictory snow aspect. Finally, for \"I hate her and I love her\" (ğŸ˜ ğŸ’•), the model generated emojis that capture the conflicting emotions, but it could have better expressed the contradiction with more emojis, such as a combination of love and anger symbols. Overall, while the model handled some contradictory prompts well, it struggled with fully capturing the complexity or contradiction in others."],"metadata":{"id":"560E8KPstu0b"}},{"cell_type":"markdown","source":["## Task 3 - Comparison with GPT 4"],"metadata":{"id":"q8mlUlYEuKov"}},{"cell_type":"markdown","source":["### ChatGPT Prompts:\n","1. **Running at the beach**  \n","   *Prompt:* Generate only emojis that symbolize running at the beach.\n","   \n","2. **I just signed a new job offer!**  \n","   *Prompt:* Respond with emojis that capture the excitement of signing a new job offer.\n","   \n","3. **Watching football with my friends.**  \n","   *Prompt:* Generate only emojis that represent watching football with friends.\n","\n","4. **Go Irish!**  \n","   *Prompt:* Respond with emojis that represent the phrase \"Go Irish!\".\n","   \n","5. **Going to win this race.**  \n","   *Prompt:* Generate only emojis that represent the feeling of going to win a race.\n","\n","### Fine-Tuned LLaMA 2 Model Outputs:\n","1. **Running at the beach**  \n","   *Output:* ğŸƒğŸŒŠğŸŒ´  \n","   \n","2. **I just signed a new job offer!**  \n","   *Output:* ğŸ’¼ğŸ‘¥ğŸ’°  \n","\n","3. **Watching football with my friends**  \n","   *Output:* ğŸˆğŸ‘¥ğŸ”  \n","\n","4. **Go Irish!**  \n","   *Output:* ğŸ€ğŸ€ğŸ€ğŸ€  \n","\n","5. **Going to win this race**  \n","   *Output:* ğŸğŸ’¨ğŸ\n","\n","### ChatGPT Outputs:\n","1. **Running at the beach**  \n","   *Output:* ğŸƒâ€â™€ï¸ğŸŒŠâ˜€ï¸  \n","\n","2. **I just signed a new job offer!**  \n","   *Output:* ğŸ–‹ï¸ğŸ’¼ğŸ‰  \n","\n","3. **Watching football with my friends**  \n","   *Output:* ğŸˆğŸºğŸ‘¯  \n","\n","4. **Go Irish!**  \n","   *Output:* ğŸ€ğŸ‡®ğŸ‡ªğŸ‰  \n","\n","5. **Going to win this race**  \n","   *Output:* ğŸğŸ’¨ğŸ†\n","\n","### Comparison:\n","\n","1. **Running at the beach:**  \n","   - **LLaMA 2:** ğŸƒğŸŒŠğŸŒ´  \n","   - **ChatGPT:** ğŸƒâ€â™€ï¸ğŸŒŠâ˜€ï¸  \n","   - **Analysis:** Both models generated appropriate emojis. LLaMA 2 used a simple and accurate set of running, beach, and nature-related emojis. ChatGPT added a sun emoji, making it more specific to a daytime beach scenario, which could be a more fitting context depending on the user's intent.  \n","   - **Quality & Relevance:** Both outputs are relevant, but ChatGPT adds more context with the sun emoji.\n","\n","2. **I just signed a new job offer:**  \n","   - **LLaMA 2:** ğŸ’¼ğŸ‘¥ğŸ’°  \n","   - **ChatGPT:** ğŸ–‹ï¸ğŸ’¼ğŸ‰  \n","   - **Analysis:** LLaMA 2 focused on work and money-related emojis, which are relevant but could be seen as more formal. ChatGPT added the pen emoji (ğŸ–‹ï¸), representing the signing aspect, and included the celebratory \"ğŸ‰,\" which adds emotional context.  \n","   - **Quality & Relevance:** Both are relevant, but ChatGPT's inclusion of the pen and celebration makes the output more fitting for the excitement of signing a job offer.\n","\n","3. **Watching football with my friends:**  \n","   - **LLaMA 2:** ğŸˆğŸ‘¥ğŸ”  \n","   - **ChatGPT:** ğŸˆğŸºğŸ‘¯  \n","   - **Analysis:** LLaMA 2 provided a more neutral, straightforward approach, focusing on football, friends, and food. ChatGPT's output included \"ğŸº,\" which emphasizes the social aspect of watching a game, and \"ğŸ‘¯,\" possibly symbolizing friendship or a group gathering.  \n","   - **Quality & Relevance:** Both outputs are good, but ChatGPT adds more specific social context with the drink and gathering emojis.\n","\n","4. **Go Irish!**  \n","   - **LLaMA 2:** ğŸ€ğŸ€ğŸ€ğŸ€  \n","   - **ChatGPT:** ğŸ€ğŸ‡®ğŸ‡ªğŸ‰  \n","   - **Analysis:** LLaMA 2 used repeated shamrocks to symbolize \"Go Irish!\" ChatGPT added more variety by including the Irish flag emoji and a celebration emoji.  \n","   - **Quality & Relevance:** LLaMA 2â€™s response is a bit repetitive, while ChatGPTâ€™s is more creative and includes a wider range of symbols.\n","\n","5. **Going to win this race:**  \n","   - **LLaMA 2:** ğŸğŸ’¨ğŸ  \n","   - **ChatGPT:** ğŸğŸ’¨ğŸ†  \n","   - **Analysis:** Both models captured the idea of racing well. LLaMA 2 used a race car, speed, and finish line, which is highly relevant. ChatGPT used the finish line, speed, and added the trophy emoji (ğŸ†), emphasizing the victory aspect.  \n","   - **Quality & Relevance:** Both outputs are strong, but ChatGPT's inclusion of the trophy better emphasizes the theme of winning.\n","\n","Both generate relevant and creative emoji sequences, though ChatGPT's outputs tend to have more context, variety, and creativity in some cases."],"metadata":{"id":"JdR83fxDuOQK"}},{"cell_type":"markdown","source":[],"metadata":{"id":"f3kAXFW6uceV"}},{"cell_type":"markdown","source":["## Task 4: Investigating Overfitting in Fine-Tuning"],"metadata":{"id":"vhbkLsvVu2Dh"}},{"cell_type":"code","source":["num_train_epochs = 5\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save trained model and tokenizer\n","trainer.model.save_pretrained(new_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337,"referenced_widgets":["ef989e32075449f795a165afc0fd8394","92930cfd5ab843cebd26a117587019ad","a012c9622df744fbab4d99d3cf1238dc","9dc2bea9271147f187465c724bb7916f","25b50f9bb52e4714beb93339dd9958d6","ded65479c6e242238a45a028114d30a4","2dedf9821a1c4a76bf62d4cf86869517","a5852142c0ed46cdb4025cc2d8becef1","09a9a777649949d1b96ea49d80395752","5709fe832ac246b092bbbd868b2a0437","46969694b3e94f419852038fb8fe618f"]},"id":"-yIuhpa_vXZH","executionInfo":{"status":"ok","timestamp":1733199741055,"user_tz":300,"elapsed":213552,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"944ec80d-64dd-4773-8d81-47b70a12ac1a"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef989e32075449f795a165afc0fd8394"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'loss': 2.9724, 'learning_rate': 0.0001975746552556772, 'epoch': 0.5}\n","{'loss': 1.0541, 'learning_rate': 0.00018550053929480202, 'epoch': 1.0}\n","{'loss': 0.4644, 'learning_rate': 0.00016449948488669639, 'epoch': 1.5}\n","{'loss': 0.2749, 'learning_rate': 0.000136764169663272, 'epoch': 2.0}\n","{'loss': 0.1908, 'learning_rate': 0.00010519038181318999, 'epoch': 2.5}\n","{'loss': 0.179, 'learning_rate': 7.307467669163655e-05, 'epoch': 3.0}\n","{'loss': 0.1649, 'learning_rate': 4.377019014049223e-05, 'epoch': 3.5}\n","{'loss': 0.1623, 'learning_rate': 2.03365443542764e-05, 'epoch': 4.0}\n","{'loss': 0.156, 'learning_rate': 5.22039891260262e-06, 'epoch': 4.5}\n","{'loss': 0.1574, 'learning_rate': 0.0, 'epoch': 5.0}\n","{'train_runtime': 210.884, 'train_samples_per_second': 4.742, 'train_steps_per_second': 1.185, 'train_loss': 0.5776116104125977, 'epoch': 5.0}\n"]}]},{"cell_type":"code","source":["pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=28)\n","\n","prompts = [\n","    'Have a great workout!',\n","    \"Running at the beach.\",\n","    \"Happy holidays!\",\n","    \"I just signed a new job offer!\",\n","    \"Iâ€™m listening to music.\"\n","]\n","\n","# Generate emoji sequences for the prompts\n","for prompt in prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    emojis = extract_emojis(result[0]['generated_text'])\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Emojis: {emojis}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbN6DBl6vhJX","executionInfo":{"status":"ok","timestamp":1733200423195,"user_tz":300,"elapsed":14290,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"5c359671-1613-4a3d-8998-68c8cb177424"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Have a great workout!\n","Generated Emojis: ğŸ’ªğŸ‹\n","\n","Prompt: Running at the beach.\n","Generated Emojis: ğŸƒğŸ–\n","\n","Prompt: Happy holidays!\n","Generated Emojis: ğŸ„ğŸ\n","\n","Prompt: I just signed a new job offer!\n","Generated Emojis: ğŸ‘¨\n","\n","Prompt: Iâ€™m listening to music.\n","Generated Emojis: ğŸ§ğŸ¶ğŸµ\n","\n"]}]},{"cell_type":"markdown","source":["After fine-tuning the model for 5 epochs, the generated emojis for the given prompts are mostly relevant but show some inconsistencies. For \"Have a great workout!\" the emojis ğŸ’ªğŸ‹ are fitting, though the addition of a person exercising might improve it. \"Running at the beach\" with ğŸƒğŸ– appropriately conveys the activity and setting. \"Happy holidays!\" with ğŸ„ğŸ captures the festive spirit well. \"I just signed a new job offer!\" is less precise with just ğŸ‘¨, missing out on symbols like a briefcase or money, which would be more relevant. Finally, \"Iâ€™m listening to music\" with ğŸ§ğŸ¶ğŸµ is spot-on. Overall, while the model produces contextually relevant emojis, some prompts could benefit from more precise or varied outputs."],"metadata":{"id":"Ppb08QCMx1uA"}},{"cell_type":"code","source":["pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n","\n","prompts = [\n","    'Explain the concept of gravity.',\n","    \"What is the capital of the USA.\",\n","    \"What is the largest planet in the solar system?\",\n","    \"Who created Instagram?\",\n","    \"What is the chemical symbol for water\"\n","]\n","\n","# Generate emoji sequences for the prompts\n","for prompt in prompts:\n","    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","    print(f\"Prompt: {prompt}\")\n","    print(f\"Generated Text: {result[0]['generated_text']}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4jOraAHx2dG","executionInfo":{"status":"ok","timestamp":1733200515536,"user_tz":300,"elapsed":83784,"user":{"displayName":"Antonio Karam","userId":"17802015091423154551"}},"outputId":"c631b210-fbf6-45b1-b296-c5a58f94b31e"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: Explain the concept of gravity.\n","Generated Text: <s>[INST] Explain the concept of gravity. [/INST] ğŸŒŸğŸ‘€ğŸŒ²ğŸŒŠğŸ›ŒğŸ˜ŒğŸ‘ğŸ¤“ğŸš€ğŸŒŸğŸŒŠğŸ³ğŸŒŸğŸ˜ğŸ‘€ğŸ¤“ğŸ›ŒğŸ˜ŒğŸ‘ğŸ¤“ğŸš€\n","\n","Prompt: What is the capital of the USA.\n","Generated Text: <s>[INST] What is the capital of the USA. [/INST] ğŸ‡ºğŸ‡¸ğŸ—ºï¸ğŸ›ï¸ğŸ‡¬ğŸ‡§\n","\n","Prompt: What is the largest planet in the solar system?\n","Generated Text: <s>[INST] What is the largest planet in the solar system? [/INST] ğŸŒğŸ‘¥ğŸš€ğŸŒŒğŸ‘€ğŸ¤”ğŸ’­ğŸŒŸğŸŒŠğŸœï¸ğŸ›ŒğŸ˜ŒğŸ‘ğŸ¤©ğŸš«ğŸ›«ğŸ‰ğŸ¤©ğŸ‘ğŸ‘Œ\n","\n","Prompt: Who created Instagram?\n","Generated Text: <s>[INST] Who created Instagram? [/INST] ğŸ“·ğŸ‘“ğŸ¤ [/INST] ğŸ“šğŸ“ğŸ¥³ [/INST] ğŸ§ğŸ®ğŸ‘¥ğŸ‘¦ğŸ¼ğŸš£ğŸ¼ğŸ›€ğŸ‘€ğŸ¤ğŸ¼ğŸ˜ [/INST]\n","\n","Prompt: What is the chemical symbol for water\n","Generated Text: <s>[INST] What is the chemical symbol for water [/INST] ğŸ’§ğŸŒŠğŸ¥‚\n"," everybody! ğŸ‰ğŸŠğŸˆğŸ‰ğŸŠğŸˆğŸ‰ğŸŠğŸˆğŸ‰ğŸŠğŸˆğŸ‰ğŸŠğŸˆğŸ‰ğŸŠ\n","\n"]}]},{"cell_type":"markdown","source":["\n","1. **Gravity :**\n","   - **Fine-Tuned Model:** The fine-tuned model generates a series of emojis that don't align with the expected explanation of gravity, such as ğŸŒŸğŸ‘€ğŸŒ², and completely fails to provide a coherent response related to the concept of gravity.\n","   - **Original Model:** The original model provides a detailed and accurate scientific explanation of gravity as a force of nature.\n","\n","The fine-tuned model strays far from the expected factual answer, producing an incoherent string of emojis instead of the informative content expected. This could indicate overfitting to the emoji generation task, causing it to default to emojis even when not appropriate.\n","<br> <br>\n","\n","2. **Capital of the USA:**\n","   - **Fine-Tuned Model:** The fine-tuned model generates emojis like ğŸ‡ºğŸ‡¸ğŸ—ºï¸ğŸ›ï¸ğŸ‡¬ğŸ‡§, which, although they include some relevant symbols (e.g., ğŸ‡ºğŸ‡¸ for the USA and ğŸ›ï¸ for a government building), fail to specify the capital, Washington, D.C.\n","   - **Original Model:** The original model provides a clear and accurate response, stating that Washington, D.C. is the capital of the USA.\n","   \n","The fine-tuned model fails to provide the correct factual information, replacing it with emojis. This suggests a loss of generalization ability, where the model no longer effectively handles factual questions after fine-tuning.\n","\n","<br><br>\n","\n","3. **Largest Planet in the Solar System:**\n","   - **Fine-Tuned Model:** The fine-tuned model outputs a string of emojis that don't convey any information about the largest planet, Jupiter. Instead, it produces a random sequence of emojis like ğŸŒğŸ‘¥ğŸš€.\n","   - **Original Model:** The original model correctly identifies Jupiter as the largest planet in the solar system and provides supporting details.\n","\n","This response highlights the model's failure to provide a coherent or useful answer post-fine-tuning, focusing instead on irrelevant emojis. This further suggests overfitting to the emoji generation task, where the model has lost its ability to process and respond appropriately to factual queries.\n","<br><br>\n","\n","4. **Who Created Instagram:**\n","   - **Fine-Tuned Model:** The fine-tuned model provides a disjointed series of emojis (e.g., ğŸ“·ğŸ‘“ğŸ¤), which don't meaningfully address the question of who created Instagram.\n","   - **Original Model:** The original model offers a comprehensive response, detailing the founders of Instagram and their timeline.\n","  \n","  The fine-tuned model's response is entirely out of context, showing that it now prioritizes emoji generation over factual accuracy, diverging from the expected behavior of providing factual answers.\n","  <br><br>\n","\n","5. **Chemical Symbol for Water:**\n","   - **Fine-Tuned Model:** The fine-tuned model produces emojis like ğŸ’§ğŸŒŠğŸ¥‚ and a long string of celebration-related emojis, which fail to provide the correct answer (H2O).\n","   - **Original Model:** The original model accurately states that the chemical symbol for water is H2O.\n","  \n","  Once again, the fine-tuned model deviates from providing accurate information, instead generating irrelevant emojis and disrupting the original model's ability to answer a straightforward question.\n","  <br>\n","  <br>\n","\n","\n","The fine-tuned model struggles to handle factual prompts, producing irrelevant or nonsensical emoji sequences instead of providing the expected, informative answers. This  contrast to the original LLaMA 2 model's performance indicates overfitting to the emoji generation task, where the model fails to generalize properly and loses its ability to provide factual or contextually appropriate responses."],"metadata":{"id":"wIHVMRdv0LBf"}},{"cell_type":"markdown","source":["\n","The fine-tuned model shows clear signs of overfitting, particularly in its inability to generate relevant text-based answers for factual prompts. Instead of providing informative responses to prompts like \"Explain the concept of gravity\" or \"What is the capital of the USA?\", the model consistently produces irrelevant emoji sequences. This suggests that the model has become too specialized in the emoji generation task and has lost its ability to generalize to other types of queries. Furthermore, the emoji sequences generated are repetitive and lack creativity, indicating that the model is overly focused on the learned emoji patterns.\n","\n","To address overfitting, several strategies could be employed. Early stopping could prevent the model from training too long, ensuring it doesn't overfit to the training data and retains its ability to generalize. Regularization techniques like L2 regularization or dropout would encourage the model to learn more generalized patterns rather than memorizing specific responses. Additionally, data augmentation could help by diversifying the training set, making the model less likely to overfit to specific emoji sequences. Finally, hyperparameter tuning could optimize the learning process, ensuring the model learns at a balanced pace and doesn't become too specialized in the emoji task. By combining these strategies, the model could be fine-tuned to improve both its performance in emoji generation and its ability to handle a variety of tasks effectively."],"metadata":{"id":"uTNcqs4d0ukh"}}]}