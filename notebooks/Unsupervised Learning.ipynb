{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0WqIWxpWNMZ"
      },
      "source": [
        "# Unsupervised Learning\n",
        "\n",
        "## Background\n",
        "This is intended to introduce you to classical approaches to unsupervised learning, using a data set that is famous in the face recognition literature.  To avoid making the class entirely dependent on scikit-learn's _fabulous_ implementations of clustering algorithms, you'll both code up your own implementation of the classical k-means algorithm, and compare it against some of sklearn's stuff.\n",
        "\n",
        "If you haven't already done so, consider bookmarking some key resources:\n",
        "+ The scikit-learn tutorial: https://scikit-learn.org/stable/tutorial/index.html , especially the unsupervised learning section (https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html)\n",
        "+ Scikit-learn's cluster analysis guide (https://scikit-learn.org/stable/modules/clustering.html). Focus on sections 2.3.1, 2.3.2, 2.3.6.{1,2}, 2.3.7, and 2.3.11.  it's a good idea to commit the sklearn approach to inference to your memory. Most of the clustering methods work by creating an object and then `fit()`ing it to some data.  This makes it easy to unplug one clustering approach and plug in another, which can be useful when you're doing exploratory data analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf2hxep9WHRa"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import tqdm.notebook as tqdm\n",
        "import numpy as np\n",
        "import scipy\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import sklearn.datasets\n",
        "import sklearn.cluster\n",
        "import sklearn.metrics\n",
        "\n",
        "import gdown\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import davies_bouldin_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pAHtq5NWwJD"
      },
      "source": [
        "## Intro\n",
        "To prepare for this assignment, please visit and bookmark some resources that will help you get up to speed on scikit-learn's features:\n",
        "\n",
        "+ The scikit-learn tutorial: https://scikit-learn.org/stable/tutorial/index.html\n",
        "+ The unsupervised learning section (https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html)\n",
        "+ Scikit-learn's cluster analysis guide (https://scikit-learn.org/stable/modules/clustering.html). Focus on sections 2.3.1, 2.3.2, 2.3.6.{1,2}, 2.3.7, and 2.3.11.\n",
        "\n",
        "It's a good idea to commit the sklearn approach to inference to your memory. Most of the clustering methods work by creating an object and then `.fit()`ing it to some data.  This makes it easy to unplug one clustering approach and plug in another, which can be useful when you're doing exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9iPGBBT4cnd"
      },
      "source": [
        "## LFW data\n",
        "\n",
        "a. Using `sklearn.datasets.fetch_lfw_people()`, get a subset of the Labeled Faces in the Wild (LFW) dataset.\n",
        " + Specify `min_faces_per_person=50` to get several classes (identities), each with 50 or more images.\n",
        " + Specify `return_X_y=True`, and assign the output of `.fetch_lfw_people()` to the tuple `lfw_pattern,lfw_label`.  Do not use different variable names, otherwie some helper code below will fail.\n",
        "\n",
        "After this is done, you'll have:\n",
        "\n",
        " + a variable `lfw_pattern`, which is a 1560 x 2914 pattern matrix (1560 patterns, 2914 features) stored in a numpy array of shape `(1560,2914)`, and\n",
        " + a variable `lfw_label` which is a numpy vector of shape `(1560,)` (which means you can only use one subscript to index it -- it's not 1560x1, it's 1560. Yes, this is weird; this is numpy.\n",
        "\n",
        "This step will take a bit of time to complete the first time you do it with a fresh runtime. Subsequent fetches are faster because the data is cached in your runtime.\n",
        "\n",
        "b. Write a bit of code to compute and print out the number of classes in this data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otaHRZrMWuux"
      },
      "outputs": [],
      "source": [
        "#a.\n",
        "lfw_pattern, lfw_label = sklearn.datasets.fetch_lfw_people(min_faces_per_person=50, return_X_y=True)\n",
        "print('Pattern:')\n",
        "print(lfw_pattern, '\\n')\n",
        "\n",
        "print('Label:')\n",
        "print(lfw_label, '\\n')\n",
        "\n",
        "#b.\n",
        "unique_classes = np.unique(lfw_label)\n",
        "num_classes = len(unique_classes)\n",
        "\n",
        "print(\"Number of classes in the dataset: \", num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEif_yT0d7Zz"
      },
      "source": [
        "## Visualize\n",
        "\n",
        "The patterns in this data set are actually images.  Each feature in the pattern is the normalized intensity of a pixel, and the pattern is a 62x47 array of pixels \"flattened\" into a 2914-element vector.\n",
        "\n",
        "Write a function `show_face(x,s)` which uses Matplotlib's `plt.imshow()` routine to render `x`, which will be one of the patterns (rows) in the LFW pattern matrix you just fetched, as a grayscale image.  Use `plt.title(s)` to add a title.\n",
        "\n",
        "Test your function by running the second code cell below. It picks an image randomly from each class and calls your code to display it.\n",
        "\n",
        "Notes:\n",
        " + you must render the image with a *grayscale* colormap so the face looks like a proper photograph. No strange colors or \"negative\" images! Look at the `cmap` option to `plt.imshow()` to accomplish this.\n",
        " + you will need to `.reshape()` the pattern supplied as `x` into a 62x47 Numpy array that you then supply to `.imshow()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic0hp6vFcFtu"
      },
      "outputs": [],
      "source": [
        "def show_face(x,s):\n",
        "\n",
        "  # Reshape pattern\n",
        "  image_data = x.reshape(62,47)\n",
        "\n",
        "  # Set grayscale\n",
        "  plt.imshow(image_data, cmap='gray')\n",
        "\n",
        "  # Set title\n",
        "  plt.title(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ci3Uy6o6x7X"
      },
      "outputs": [],
      "source": [
        "for l in set(lfw_label):\n",
        "    faces = [x for i,x in enumerate(lfw_pattern) if lfw_label[i] == l]\n",
        "    face = random.choice(faces)\n",
        "    show_face(face,f'Class {l}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHgLGQ9Z-h7s"
      },
      "source": [
        "# $k$-means\n",
        "\n",
        "In the cells below, write a function `my_k_means(pattern,k,max_iterations=10,dist=scipy.spatial.distance.euclidean)` that implements the famous $k$-means clustering algorithm.  The parameters to the function are:\n",
        " + the pattern matrix `pattern` with rows as patterns and columns as features.\n",
        " + `k` -- the desired maximum number of clusters to return.\n",
        " + `max_iterations` -- the maximum number of iterations of the $k$-means loop that will occur before the algorithm stops trying to improve the clustering results. This is a keyword argument with a default value of 10.\n",
        " + `dist` -- the name of a function that will compute the distance between two patterns supplied as arguments. This is a keyword argument with a default value of `scipy.spatial.distance.euclidean` (which, despite the oh-so-fancy name, just computes Euclidean distance).\n",
        "\n",
        "The output of the algorithm is a tuple of two items:\n",
        " + a label vector (use a numpy vector for this), that is the same length as the number of patterns, with each element being an integer between 0 and $k-1$, inclusive. Note: the algorithm may not use every label in its output (_i.e._, you could specify $k=4$ and get a result where only labels 0, 2, and 3 are used), and\n",
        " + The **sum of squared distances** (SSD) between the patterns and their assigned cluster center.\n",
        "\n",
        "Your code should follow the following basic outline, which is provided in the code cell below:\n",
        "\n",
        " + Choose $k$ cluster centers at random from the data (Hint: `random.choice()`\n",
        " + Compute \"current\" cluster assignments -- label each pattern with the index of its closest cluster center:\n",
        "  + use the `dist` function to calculate the distances between a pattern `x` and each of the cluster centers.\n",
        "  + the index of the closest cluster center for `x` is the initial label for `x`. Hint: `np.argmin()`\n",
        " + Loop no more than `max_iterations` times\n",
        "  + Update cluster centers -- the new cluster center for cluster $i$ is the centroid (_i.e._, mean or average) of all the patterns assigned to it. Hint: `np.mean()`\n",
        "  + Compute \"new\" cluster assignments (labels) and compare them to the \"current\" ones\n",
        "    + if no labels changed, break out of the loop\n",
        "    + if one or more labels changed, make \"current\" assignments equal to \"new\" assignments\n",
        " + When loop ends, calculate SSD by adding up the squared distances between all patterns and their nearest cluster center.\n",
        " + return cluster assignments and SSD\n",
        "\n",
        "Put all your $k$-means code in the code cell below. Your are welcome (and encouraged!) to define helper functions, especially if they make the \"core\" of the $k$-means algorithm more compact and readable. Hint: As specified above, you will perform pattern assignment in two places. Maybe the same function can be called twice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9t2bYxnlzXo"
      },
      "outputs": [],
      "source": [
        "def my_k_means(pattern,k,max_iterations=10,dist=scipy.spatial.distance.euclidean):\n",
        "  # Choose starting cluster centers at random\n",
        "  indices = np.random.permutation(pattern.shape[0])\n",
        "  centers = pattern[indices[:k], :]\n",
        "\n",
        "  # Perform initial cluster membership assignments\n",
        "  labels = np.argmin([np.apply_along_axis(dist, 1, pattern, center) for center in centers], axis=0)\n",
        "\n",
        "  # Loop no more than max_iterations\n",
        "  for _ in range(max_iterations):\n",
        "    #  Update cluster centers using all pattern assignments\n",
        "    centers = np.array([pattern[labels == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "    #  Calculate new assgnments\n",
        "    new_labels = np.argmin([np.apply_along_axis(dist, 1, pattern, center) for center in centers], axis=0)\n",
        "\n",
        "    if np.all(labels == new_labels):\n",
        "      break\n",
        "    else:\n",
        "      labels = new_labels\n",
        "\n",
        "  # Calculate SSD\n",
        "  ssd = sum(dist(pattern[i], centers[labels[i]])**2 for i in range(pattern.shape[0]))\n",
        "\n",
        "  # Return final set of assignments and SSD\n",
        "  return (labels, ssd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tp6Rjdj5NMq"
      },
      "outputs": [],
      "source": [
        "pattern,label = sklearn.datasets.make_blobs(n_samples=100,centers=2, n_features=2)\n",
        "\n",
        "fig,(ax1,ax2) = plt.subplots(1,2)\n",
        "ax1.set_title('Ground Truth labeling')\n",
        "ax2.set_title('k-means labeling')\n",
        "_ = ax1.scatter(pattern[:,0],pattern[:,1],c=label)\n",
        "\n",
        "(clabel,ssd) = my_k_means(pattern,k=2)\n",
        "print(f'GT labels:      {label}')\n",
        "print(f'k-means labels: {clabel}')\n",
        "\n",
        "_ = ax2.scatter(pattern[:,0],pattern[:,1],c=clabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-m4B4c84cfT"
      },
      "source": [
        "# Clustering evaluation\n",
        "\n",
        "Apply your $k$-means implementation  to the LFW data.  Your goal is a clustering that minimizes SSD. Since $k$-means picks its initial cluster centers at random, different runs with the same input data can end up with different results! Generally, $k$-means is run multiple times and the resulting labeling with the lowest SSD is the winner.\n",
        "\n",
        "Search for values of $k$ between 8 and 20. For each value of $k$, run $k$-means ten times with `max_iterations` set to 100. Note: my implementation of $k$-means, not particularly optimized, took 20 minutes to run these 13 different values of $k$. Plot the SSD for each run at each value of $k$ as a dot on a single plot. On the same plot, provide a line plot of the minimum SSD.\n",
        "\n",
        "It is *extremely* likely that your plot of minimum SSD versus k will show a consistent drop as k increases. In the text box below your code and the plot you generate, discuss why this is so. Hint: what would the SSD be if each pattern was in its own cluster (_i.e._, $k$ = the number of patterns)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9BDaaV8m0xj"
      },
      "outputs": [],
      "source": [
        "# Arrays to store SSD values\n",
        "ssd_vals = []\n",
        "ssd_vals_min = []\n",
        "\n",
        "# Range of k values to test\n",
        "k_values = range(8, 21)\n",
        "\n",
        "# Define a list of colors\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "\n",
        "# For each k value...\n",
        "for i, k in enumerate(k_values):\n",
        "    # Initialize list to store SSDs for this k\n",
        "    k_ssds = []\n",
        "\n",
        "    # Run k-means 10 times\n",
        "    for _ in range(10):\n",
        "        _, ssd = my_k_means(lfw_pattern, k=k, max_iterations=100)\n",
        "        k_ssds.append(ssd)\n",
        "\n",
        "    # Add SSDs for this k to master list\n",
        "    ssd_vals.extend(k_ssds)\n",
        "\n",
        "    # Calculate and store minimum SSD for this k\n",
        "    ssd_vals_min.append(min(k_ssds))\n",
        "\n",
        "    # Create scatter plot of SSDs with color corresponding to k\n",
        "    plt.scatter([k]*10, k_ssds, color=colors[i % len(colors)])\n",
        "\n",
        "\n",
        "# Create line plot of minimum SSDs\n",
        "plt.plot(k_values, ssd_vals_min, label='Minimum SSD', color='red')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('K value')\n",
        "plt.ylabel('SSD value')\n",
        "plt.title(\"Homemade, slow, painful model\")\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5uD-pk5U0dr"
      },
      "source": [
        "# Sklearn\n",
        "\n",
        "Read the documentation for sklearn's implementation of $k$-means [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
        "Repeat the previous experiment but use `sklearn.cluster.KMeans` instead of your own code to compute the k-means clustering. To maximize the apples-to-apples comparison, specify these parameters (in addition to a value of $k$) when you create the `KMeans` object:\n",
        " + `init='random'`\n",
        " + `n_init=1`\n",
        " + `max_iter=100`, and\n",
        " + `tol=1.0e-30`.\n",
        "\n",
        "Note: with `n_init=1`, only one run of $k$-means is performed each time you call it - so you still need to have an inner loop that runs $k$-means 10 times.\n",
        "\n",
        "Another note: This should run much faster than your home-brewed version of $k$-means.\n",
        "\n",
        "Compute and show the same graph you computed/showed in Problem 4. Read the documentation to figure out how to get the output labels and the SSD from the `KMeans` object.\n",
        "\n",
        " In the text box below the code box, comment on anything you observed with regard to results, performance (run time), _etc_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXeQNdwFWw2Q"
      },
      "outputs": [],
      "source": [
        "# Arrays to store SSD values\n",
        "sk_ssd_vals = []\n",
        "sk_ssd_vals_min = []\n",
        "\n",
        "# Range of k values to test\n",
        "k_values = range(8, 21)\n",
        "\n",
        "# Define a list of colors\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "\n",
        "# For each k value...\n",
        "for i, k in enumerate(k_values):\n",
        "    # Initialize list to store SSDs for this k\n",
        "    k_ssds = []\n",
        "\n",
        "    # Run k-means 10 times\n",
        "    for _ in range(10):\n",
        "        model = sklearn.cluster.KMeans(n_clusters=k, init='random', n_init=1, max_iter=100, tol=1.0e-30)\n",
        "        model.fit(lfw_pattern)\n",
        "        k_ssds.append(model.inertia_)\n",
        "\n",
        "    # Add SSDs for this k to master list\n",
        "    sk_ssd_vals.extend(k_ssds)\n",
        "\n",
        "    # Calculate and store minimum SSD for this k\n",
        "    sk_ssd_vals_min.append(min(k_ssds))\n",
        "\n",
        "    # Create scatter plot    of SSDs with color corresponding to k\n",
        "    plt.scatter([k]*10, k_ssds, color=colors[i % len(colors)])\n",
        "\n",
        "\n",
        "# Create line plot of minimum SSDs\n",
        "plt.plot(k_values, sk_ssd_vals_min, label='Minimum SSD', color='red')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('K value')\n",
        "plt.ylabel('SSD value')\n",
        "plt.title('Sklearn KMeans Model')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfS0-MuDDRSR"
      },
      "source": [
        "# Clustering data with no labels\n",
        "\n",
        "This is the typical use case for clustering - you just have data, and want to learn something about its structure. Cluster analysis will help you uncover groupings.\n",
        "\n",
        "Clustering metrics are often used to discern the proper number of clusters.\n",
        "\n",
        "This problem asks you to evaluate two clustering algorithms' outputs on a dataset using two different clustering metrics.\n",
        "\n",
        "But first, you need data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Zw_GKKudbe"
      },
      "outputs": [],
      "source": [
        "lnks = ['1ZHHf0DrwFS7FF5qR4AhjBmSnb4EQmVK9','1ZIjxI6Cb0oh2Pm6t1RNIdyMxHIfo8bid', \\\n",
        "        '1ZJPzyn6VXOUYNMOPjrZxjaiPQu5atBQV','1ZJqX0_ZzcFsDKJjrdrCT-F3-gTL1TYO5', \\\n",
        "        '1ZKAMdRcjS86KGZVxnIn57_fYpXnC1Ulg','1ZKkLpPnfiEI5-wqDd9C4wRw5Us8B7HX8', \\\n",
        "        '1ZKx5QbCRUSzT0siI4NeMrK_MTq355gMN','1ZLRr57fSuddUcyVN-tSffFrI1EBAdbfv', \\\n",
        "        '1ZMOF5beH5x3yl8Q4jHu0zxEVFpM3Hkie','1ZMcXxWiGLOOkWdF9KUH6ZlyuQbNerehF']\n",
        "\n",
        "def geturl(digit):\n",
        "    \"\"\"returns a gdown-compatible URL for one of ten shared files.\"\"\"\n",
        "    pfx = 'https://drive.google.com/uc?id='\n",
        "    # guard code\n",
        "    if type(digit) is str: digit = int(digit)\n",
        "    digit = digit % 10\n",
        "    return pfx + lnks[digit]\n",
        "\n",
        "MY_DIGIT = '3'\n",
        "\n",
        "url = geturl(MY_DIGIT)\n",
        "#print(url)\n",
        "if os.path.isfile('mydata.pkl'):\n",
        "    os.remove('mydata.pkl')\n",
        "if not os.path.exists('mydata.pkl'):\n",
        "    gdown.download(url,'mydata.pkl')\n",
        "print('mydata.pkl is available in the runtime.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEUQDf5XO_nF"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "fp = open('mydata.pkl', 'rb')\n",
        "pattern_matrix = pickle.load(fp)\n",
        "print(\"Pattern Matrix:\")\n",
        "print(pattern_matrix.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5nU54HY231"
      },
      "source": [
        "There are between two and 15 clusters in your data. Use k-means to investigate this.\n",
        "\n",
        "Run k-means for the range of `k` values suggested above (2 to 15 clusters). Look at the output of the homogeneity and Davies-Bouldin scores (make sure you know what \"better\" values are for both scores), and see if one or more values of `k` suggest themselves as potential good values for the true number of clusters. In the text cell below your code cell, explain your reasoning. Note: this problem will not be graded on whether you get the correct value - it will be based on the quality of your argument.\n",
        "+ Documentation on Davies-Bouldin score computation in sklearn [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html).\n",
        "+ Documentation on homogeneity score computation in sklearn [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYfEvCMSQ2WA"
      },
      "outputs": [],
      "source": [
        "# Range of k values to test\n",
        "k_values = range(2, 16)\n",
        "\n",
        "# Initialize list to store Davies-Bouldin scores for each k\n",
        "db_scores = []\n",
        "\n",
        "# For each k value...\n",
        "for k in k_values:\n",
        "    # Run k-means\n",
        "    kmeans = sklearn.cluster.KMeans(n_clusters=k, n_init=1, random_state=0).fit(pattern_matrix)\n",
        "\n",
        "    # Compute Davies-Bouldin score\n",
        "    db_score = sklearn.metrics.davies_bouldin_score(pattern_matrix, kmeans.labels_)\n",
        "\n",
        "    # Add score to list\n",
        "    db_scores.append(db_score)\n",
        "\n",
        "# Create line plot of Davies-Bouldin scores\n",
        "plt.plot(k_values, db_scores, marker='o')\n",
        "\n",
        "# Add labels\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.title('Davies-Bouldin Score for different k values')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTSvcc5PchGO"
      },
      "source": [
        "Repeat question 5(b), but use the DBSCAN clustering algorithm insead of $k$-means (documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)).\n",
        " + You'll need to play with the `eps` parameter to find a good value for the number of clusters. Values for `eps` between 0.5 and 2.0 are realistic.\n",
        " + If you get **any* -1 values in the output `labels_` attribute of the `DBSCAN` object, ignore that result and increase `eps`.\n",
        "\n",
        "For each run of DBSCAN which does not yield any -1 labels, calculate the silhouette and Davies-Bouldin measures. See if one or more values of k suggest themselves as potential good values for the true number of clusters. Explain your reasoning in the text cel below the code cell. Note: this problem will not be graded on whether you get the correct value - it will be based on the quality of your argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieSCyQtkdMVq"
      },
      "outputs": [],
      "source": [
        "# Range of k values to test\n",
        "eps_values = np.arange(0.5,2.0, 0.01)\n",
        "\n",
        "# Initialize list to store Davies-Bouldin scores for each k\n",
        "db_scores = []\n",
        "valid_eps = []\n",
        "\n",
        "# For each eps value...\n",
        "for eps in eps_values:\n",
        "    # Run DBSCAN\n",
        "    dbscan = DBSCAN(eps=eps).fit(pattern_matrix)\n",
        "\n",
        "    # Check if any labels are -1 (indicating noise)\n",
        "    if -1 in dbscan.labels_:\n",
        "        continue\n",
        "\n",
        "    # Check if only one cluster is found\n",
        "    if len(set(dbscan.labels_)) <= 1:\n",
        "        continue\n",
        "\n",
        "    # Compute Davies-Bouldin score\n",
        "    db_score = davies_bouldin_score(pattern_matrix, dbscan.labels_)\n",
        "\n",
        "    # Add score to list\n",
        "    db_scores.append(db_score)\n",
        "    valid_eps.append(eps)\n",
        "    print(eps, db_score)\n",
        "\n",
        "# Create line plot of Davies-Bouldin scores\n",
        "plt.plot(valid_eps, db_scores, marker='o')\n",
        "\n",
        "# Add labels\n",
        "plt.xlabel('eps')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.title('Davies-Bouldin Score for different eps values')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
